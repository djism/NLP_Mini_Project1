{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "V28"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "TPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "3.1 (8 points) Implement the capability of computing PMIs. Use your code to calculate PMIs for w = 3 when using vocab-15kws.txt to populate V and vocab-5k.txt to populate VC. Note that since we are using different vocabularies for center words and context words, pmi(a, b) will not necessarily equal pmi(b, a) (though they will be similar). (If there is a word in V that has no counts, the numerator and denominator for all of its PMI values will be zero, so you can just define all such PMIs to be zero.) For center word x = “coffee”, print the 10 context words with the largest PMIs and the 10 context words with the smallest PMIs. Print both the words and the PMI values"
   ],
   "metadata": {
    "id": "7RyusK2wc9j5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from scipy.stats import rankdata\n",
    "\n",
    "# function to compute Spearman correlation.\n",
    "def spearman_rank_correlation(predicted_similarities, actual_scores):\n",
    "\n",
    "    predicted = [predicted_similarities[pair] for pair in actual_scores if pair in predicted_similarities]\n",
    "    actual = [actual_scores[pair] for pair in actual_scores if pair in predicted_similarities]\n",
    "\n",
    "    predicted_ranks = rankdata(predicted)\n",
    "    actual_ranks = rankdata(actual)\n",
    "\n",
    "    n = len(predicted_ranks)\n",
    "    if n == 0:\n",
    "        return None\n",
    "\n",
    "    rank_differences_squared = [(pred_r - act_r) ** 2 for pred_r, act_r in zip(predicted_ranks, actual_ranks)]\n",
    "    spearman_rho = 1 - (6 * sum(rank_differences_squared)) / (n * (n**2 - 1))\n",
    "\n",
    "    return spearman_rho"
   ],
   "metadata": {
    "id": "s17R1pBHYQF7"
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2cDPStByXTD2",
    "outputId": "223759db-bbd0-48e9-99d9-d74ac28a78c3"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Highest PMI context words for 'coffee':-\n",
      "tea:-> 8.166001262432944\n",
      "drinking:-> 7.5879786587319416\n",
      "shop:-> 7.411693771493206\n",
      "costa:-> 7.350256393786174\n",
      "shops:-> 7.2607518734184815\n",
      "sugar:-> 6.533949521544224\n",
      "coffee:-> 6.50197713180594\n",
      "mix:-> 6.131195903101994\n",
      "seattle:-> 5.950816325067406\n",
      "houses:-> 5.868161497268194\n",
      "\n",
      "Lowest PMI context words for 'coffee':-\n",
      "page:-> -1.2805627423999117\n",
      "when:-> -1.4043486976804662\n",
      "more:-> -1.478525792288141\n",
      "after:-> -1.598505205572077\n",
      "its:-> -1.839457915441183\n",
      "not:-> -1.9115928402013347\n",
      "this:-> -1.9795498179341677\n",
      "had:-> -1.9875291676196636\n",
      "be:-> -2.1509730526874753\n",
      "he:-> -2.260338264952694\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "def find_indexes(arr, words_set):\n",
    "    return [(i, w) for i, w in enumerate(arr) if w in words_set]\n",
    "\n",
    "\n",
    "with open(\"/vocab-15kws.txt\") as f:\n",
    "    vocab = f.read().split()\n",
    "\n",
    "with open(\"/vocab-5k.txt\") as f1:\n",
    "    vocab_context = f1.read().split()\n",
    "\n",
    "V = set(vocab)\n",
    "Vc = set(vocab_context)\n",
    "\n",
    "\n",
    "my_dict = defaultdict(int)\n",
    "\n",
    "\n",
    "with open(\"/wiki-1percent.txt\", \"r\") as file:\n",
    "    for line in file:\n",
    "\n",
    "        words = line.split()\n",
    "        length = len(words)\n",
    "\n",
    "        word_indexes = find_indexes(words, V)\n",
    "\n",
    "\n",
    "        for idx, word in word_indexes:\n",
    "            ind_l = max(0, idx - 3)\n",
    "            ind_H = min(length, idx + 4)\n",
    "            context_window = words[ind_l:idx] + words[idx + 1:ind_H]\n",
    "            for context_word in context_window:\n",
    "                if context_word in Vc:\n",
    "                    key = (word, context_word)\n",
    "                    my_dict[key] += 1\n",
    "\n",
    "# To add #(x,y) for all x and y.\n",
    "total_count_N = sum(my_dict.values())\n",
    "\n",
    "# To compute joint probabilities of (x,y). \n",
    "joint_probabilities = defaultdict(float)\n",
    "for (word, context_word), count in my_dict.items():\n",
    "    joint_probabilities[(word, context_word)] = count / total_count_N\n",
    "\n",
    "# To compute partial probability of x.\n",
    "partial_probability_x = defaultdict(float)\n",
    "for word in V:\n",
    "    partial_probability_x[word] = sum(joint_probabilities[(word, context_word)] for context_word in Vc)\n",
    "\n",
    "# To compute partial probability of x.\n",
    "partial_probability_y = defaultdict(float)\n",
    "for context_word in Vc:\n",
    "    partial_probability_y[context_word] = sum(joint_probabilities[(word, context_word)] for word in V)\n",
    "\n",
    "# To calculate PMI.\n",
    "pointwise_mutual_information = defaultdict(float)\n",
    "for (word, context_word), joint_prob in joint_probabilities.items():\n",
    "    partial_prob_x = partial_probability_x[word]\n",
    "    partial_prob_y = partial_probability_y[context_word]\n",
    "\n",
    "    if joint_prob > 0 and partial_prob_x > 0 and partial_prob_y > 0:\n",
    "        pointwise_mutual_information[(word, context_word)] = math.log2(joint_prob / (partial_prob_x * partial_prob_y))\n",
    "    else:\n",
    "        pointwise_mutual_information[(word, context_word)] = 0\n",
    "\n",
    "coffee_pmi = {context_word: pmi for (word, context_word), pmi in pointwise_mutual_information.items() if word == 'coffee'}\n",
    "\n",
    "sorted_pmi = sorted(coffee_pmi.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "print(\"Highest PMI context words for 'coffee':-\")\n",
    "for word, pmi in sorted_pmi[:10]:\n",
    "    print(f\"{word}:-> {pmi}\")\n",
    "\n",
    "print(\"\\nLowest PMI context words for 'coffee':-\")\n",
    "for word, pmi in sorted_pmi[-10:]:\n",
    "    print(f\"{word}:-> {pmi}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "3.2 (6 points) Now, define word vectors using PMI. That is, the word vector for a word x ∈ V has an\n",
    "entry for each word y ∈ VC with value given by pmi(x, y). As above, use w = 3, vocab-15kws.txt\n",
    "to populate V , and vocab-5k.txt to populate VC. Evaluate (EVALWS) your PMI-based word vectors\n",
    "and report your results.\n"
   ],
   "metadata": {
    "id": "FXJCHnpwdQey"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "word_vectors = defaultdict(dict)\n",
    "\n",
    "# Load word vectors with PMI value.\n",
    "for (word, context_word), pmi in pointwise_mutual_information.items():\n",
    "    word_vectors[word][context_word] = pmi"
   ],
   "metadata": {
    "id": "nqXSztrjXzM9"
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "For men dataset"
   ],
   "metadata": {
    "id": "sak7LsX3dTz1"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "word_pairs = []\n",
    "\n",
    "word_pair_scores = {}\n",
    "\n",
    "# To find word pairs and scores from given dataset.\n",
    "with open(\"/men.txt\") as f3:\n",
    "    next(f3)\n",
    "\n",
    "    for line in f3:\n",
    "        word1, word2, score = line.split()\n",
    "        pair = (word1, word2)\n",
    "        word_pairs.append(pair)\n",
    "        word_pair_scores[pair] = float(score)"
   ],
   "metadata": {
    "id": "zxU6zowMX2lN"
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "# Function to compute cosine similarity.\n",
    "def cosine_similarity(vec1, vec2):\n",
    "\n",
    "    dot_product = sum(vec1[word] * vec2[word] for word in vec1 if word in vec2)\n",
    "\n",
    "    norm_vec1 = math.sqrt(sum(value ** 2 for value in vec1.values()))\n",
    "\n",
    "    norm_vec2 = math.sqrt(sum(value ** 2 for value in vec2.values()))\n",
    "\n",
    "    if norm_vec1 == 0 or norm_vec2 == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "cosine_similarities_men = {}\n",
    "\n",
    "for (word1, word2) in word_pairs:\n",
    "    if word1 in word_vectors and word2 in word_vectors:\n",
    "\n",
    "        vec1 = word_vectors[word1]\n",
    "        vec2 = word_vectors[word2]\n",
    "\n",
    "        similarity = cosine_similarity(vec1, vec2)\n",
    "        cosine_similarities_men[(word1, word2)] = similarity"
   ],
   "metadata": {
    "id": "86ojqyqrX6OL"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "spearman_rho = spearman_rank_correlation(cosine_similarities_men, word_pair_scores)\n",
    "print(f\"Spearman's ρ for MEN dataset for w = 3: {spearman_rho}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D3xDJThvYCFs",
    "outputId": "0b767a39-9c95-4a94-85b3-f8f4cc8a4f44"
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Spearman's ρ for MEN dataset for w = 3: 0.46578947742105303\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "For simlex-999 dataset"
   ],
   "metadata": {
    "id": "Et15F-URdX_p"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "word_pairs = []\n",
    "word_pair_scores = {}\n",
    "\n",
    "with open(\"/simlex-999.txt\") as f4:\n",
    "    next(f4)\n",
    "    for line in f4:\n",
    "        word1, word2, simlex999 = line.split()\n",
    "        pair = (word1, word2)\n",
    "        word_pairs.append(pair)\n",
    "        word_pair_scores[pair] = float(simlex999)"
   ],
   "metadata": {
    "id": "9AO1vnZcYDGP"
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "cosine_similarities_simlex999 = {}\n",
    "\n",
    "for (word1, word2) in word_pairs:\n",
    "    if word1 in word_vectors and word2 in word_vectors:\n",
    "        vec1 = word_vectors[word1]\n",
    "        vec2 = word_vectors[word2]\n",
    "\n",
    "        similarity = cosine_similarity(vec1, vec2)\n",
    "\n",
    "        cosine_similarities_simlex999[(word1, word2)] = similarity"
   ],
   "metadata": {
    "id": "30g7s4O2YJLb"
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "spearman_rho = spearman_rank_correlation(cosine_similarities_simlex999, word_pair_scores)\n",
    "print(f\"Spearman's ρ for simlex-999 dataset for w = 3: {spearman_rho}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VqFERDLOYL8f",
    "outputId": "2145ce41-92a6-494b-f985-e1619b6ab40b"
   },
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Spearman's ρ for simlex-999 dataset for w = 3: 0.18644573531447284\n"
     ]
    }
   ]
  }
 ]
}
