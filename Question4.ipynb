{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "4.1 (8 points) Evaluate the word vectors (EVALWS) corresponding to the three ways of computing vectors (counts, IDF, and PMI), three values of w (1, 3, and 6), and two context vocabularies\n",
    "(vocab-15kws.txt and vocab-5k.txt). For all cases, use vocab-15kws.txt for V . Report the\n",
    "results (there should be 36 correlations in all) and describe your findings. What happens as window\n",
    "size changes for different methods of creating word vectors? What happens when context vocabulary\n",
    "changes? Why do you think you observe the trends you see? Do you see the same trends for MEN and\n",
    "SimLex or do they differ?\n",
    "\n",
    "For Count method"
   ],
   "metadata": {
    "id": "reJzwZc22I_e"
   }
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "# Function to find indices for middle word.\n",
    "def find_indexes(arr, words_set):\n",
    "    return [(i, w) for i, w in enumerate(arr) if w in words_set]\n",
    "\n",
    "# Function to find count for word vectors.\n",
    "def word_vector_count(w, vocab, vocab_context, lines):\n",
    "    V = set(vocab)\n",
    "    Vc = set(vocab_context)\n",
    "\n",
    "    my_dict = defaultdict(int)\n",
    "\n",
    "    for line in lines:\n",
    "        words = line.split()\n",
    "        length = len(words)\n",
    "        word_indexes = find_indexes(words, V)\n",
    "\n",
    "        for idx, word in word_indexes:\n",
    "            ind_l = max(0, idx - w)\n",
    "            ind_h = min(length, idx + w + 1)\n",
    "            context_window = words[ind_l:idx] + words[idx + 1:ind_h]\n",
    "\n",
    "            for context_word in context_window:\n",
    "                if context_word in Vc:\n",
    "                    my_dict[(word, context_word)] += 1\n",
    "\n",
    "    return my_dict"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "from scipy.stats import rankdata\n",
    "# Function to deduce word pairs and their scores from given dataset.\n",
    "def make_word_pairs(path):\n",
    "  word_pairs = []\n",
    "\n",
    "  word_pair_scores = {}\n",
    "  with open(path) as f3:\n",
    "      next(f3)\n",
    "      for line in f3:\n",
    "          word1, word2, score = line.split()\n",
    "          pair = (word1, word2)\n",
    "          word_pairs.append(pair)\n",
    "\n",
    "          word_pair_scores[pair] = float(score)\n",
    "\n",
    "  return word_pairs, word_pair_scores"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "# Function to calculate cosine similarity.\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = sum(vec1[word] * vec2.get(word, 0) for word in vec1)\n",
    "    norm_vec1 = math.sqrt(sum(value ** 2 for value in vec1.values()))\n",
    "    norm_vec2 = math.sqrt(sum(value ** 2 for value in vec2.values()))\n",
    "\n",
    "    if norm_vec1 == 0 or norm_vec2 == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return dot_product / (norm_vec1 * norm_vec2)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "\n",
    "# Function to calculate Spearman correlation.\n",
    "def spearman_rank_correlation(predicted_similarities, actual_scores):\n",
    "    predicted = [predicted_similarities[pair] for pair in actual_scores if pair in predicted_similarities]\n",
    "    actual = [actual_scores[pair] for pair in actual_scores if pair in predicted_similarities]\n",
    "\n",
    "    if not predicted:\n",
    "        return None\n",
    "\n",
    "    predicted_ranks = rankdata(predicted)\n",
    "    actual_ranks = rankdata(actual)\n",
    "\n",
    "    n = len(predicted_ranks)\n",
    "    rank_differences_squared = [(pred_r - act_r) ** 2 for pred_r, act_r in zip(predicted_ranks, actual_ranks)]\n",
    "\n",
    "    return 1 - (6 * sum(rank_differences_squared)) / (n * (n**2 - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RkOGQtoo8R92",
    "outputId": "57576587-2180-4e7d-ae13-b0970dd99029"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman's ρ for path = /men.txt, context_vocab = /vocab-15kws.txt, w = 1, method = count: 0.2066309206256578\n",
      "Spearman's ρ for path = /men.txt, context_vocab = /vocab-15kws.txt, w = 3, method = count: 0.22100748000083115\n",
      "Spearman's ρ for path = /men.txt, context_vocab = /vocab-15kws.txt, w = 6, method = count: 0.2371355346817261\n",
      "Spearman's ρ for path = /men.txt, context_vocab = /vocab-5k.txt, w = 1, method = count: 0.20932403959155998\n",
      "Spearman's ρ for path = /men.txt, context_vocab = /vocab-5k.txt, w = 3, method = count: 0.22536738248526467\n",
      "Spearman's ρ for path = /men.txt, context_vocab = /vocab-5k.txt, w = 6, method = count: 0.2412897444766383\n",
      "Spearman's ρ for path = /simlex-999.txt, context_vocab = /vocab-15kws.txt, w = 1, method = count: 0.07002953354155761\n",
      "Spearman's ρ for path = /simlex-999.txt, context_vocab = /vocab-15kws.txt, w = 3, method = count: 0.05715835575054007\n",
      "Spearman's ρ for path = /simlex-999.txt, context_vocab = /vocab-15kws.txt, w = 6, method = count: 0.04067000968904777\n",
      "Spearman's ρ for path = /simlex-999.txt, context_vocab = /vocab-5k.txt, w = 1, method = count: 0.06780157612522342\n",
      "Spearman's ρ for path = /simlex-999.txt, context_vocab = /vocab-5k.txt, w = 3, method = count: 0.058777383596020916\n",
      "Spearman's ρ for path = /simlex-999.txt, context_vocab = /vocab-5k.txt, w = 6, method = count: 0.044712033676963525\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "def load_file(path):\n",
    "    with open(path, \"r\") as file:\n",
    "        return file.read().splitlines()\n",
    "\n",
    "def count_lines(Vc, lines):\n",
    "    total_lines = 0\n",
    "    context_word_line_counts = defaultdict(int)\n",
    "\n",
    "    for line in lines:\n",
    "        total_lines += 1\n",
    "        words = set(line.split())\n",
    "        for word in words:\n",
    "            if word in Vc:\n",
    "                context_word_line_counts[word] += 1\n",
    "\n",
    "    return total_lines, context_word_line_counts\n",
    "\n",
    "# Defining context window size, different paths, different context word vocab and different methods.\n",
    "context_window_sizes = [1, 3, 6]\n",
    "word_pairs_paths = [\"/men.txt\", \"/simlex-999.txt\"]\n",
    "context_vocab_paths = [\"/vocab-15kws.txt\", \"/vocab-5k.txt\"]\n",
    "methods = {\"count\", \"tf-idf\", \"pmi\"}\n",
    "\n",
    "wiki_lines = load_file(\"/wiki-1percent.txt\")\n",
    "\n",
    "for path in word_pairs_paths:\n",
    "    word_pairs, word_pair_scores = make_word_pairs(path)\n",
    "\n",
    "    for context_vocab in context_vocab_paths:\n",
    "        vocab_context = load_file(context_vocab)\n",
    "\n",
    "        for w in context_window_sizes:\n",
    "            word_vectors = defaultdict(dict)\n",
    "            vocab = load_file(\"/vocab-15kws.txt\")\n",
    "            my_dict = word_vector_count(w, vocab, vocab_context, wiki_lines)\n",
    "            for (word, context_word), count in my_dict.items():\n",
    "                word_vectors[word][context_word] = count\n",
    "\n",
    "\n",
    "            cosine_similarities = {pair: cosine_similarity(word_vectors.get(pair[0], {}), word_vectors.get(pair[1], {}))\n",
    "                                   for pair in word_pairs}\n",
    "\n",
    "            spearman_rho = spearman_rank_correlation(cosine_similarities, word_pair_scores)\n",
    "            print(f\"Spearman's ρ for path = {path}, context_vocab = {context_vocab}, w = {w}, method = count: {spearman_rho}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "4.1) For TF-IDF Method"
   ],
   "metadata": {
    "id": "Mq8nixEU8QLK"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R_YTK9Go9AxY",
    "outputId": "98b45d31-c85b-410e-e8d6-15cccfcd0bad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman's ρ for path = /men.txt, context_vocab = /vocab-15kws.txt, w = 1, method = tf-idf: 0.3663543634838181\n",
      "Spearman's ρ for path = /men.txt, context_vocab = /vocab-15kws.txt, w = 3, method = tf-idf: 0.48110488290054254\n",
      "Spearman's ρ for path = /men.txt, context_vocab = /vocab-15kws.txt, w = 6, method = tf-idf: 0.5252486262498474\n",
      "Spearman's ρ for path = /men.txt, context_vocab = /vocab-5k.txt, w = 1, method = tf-idf: 0.3477510394167822\n",
      "Spearman's ρ for path = /men.txt, context_vocab = /vocab-5k.txt, w = 3, method = tf-idf: 0.47297401866377986\n",
      "Spearman's ρ for path = /men.txt, context_vocab = /vocab-5k.txt, w = 6, method = tf-idf: 0.5325364850596095\n",
      "Spearman's ρ for path = /simlex-999.txt, context_vocab = /vocab-15kws.txt, w = 1, method = tf-idf: 0.18721373377385397\n",
      "Spearman's ρ for path = /simlex-999.txt, context_vocab = /vocab-15kws.txt, w = 3, method = tf-idf: 0.14786738341547956\n",
      "Spearman's ρ for path = /simlex-999.txt, context_vocab = /vocab-15kws.txt, w = 6, method = tf-idf: 0.10879719499058171\n",
      "Spearman's ρ for path = /simlex-999.txt, context_vocab = /vocab-5k.txt, w = 1, method = tf-idf: 0.1892425842676344\n",
      "Spearman's ρ for path = /simlex-999.txt, context_vocab = /vocab-5k.txt, w = 3, method = tf-idf: 0.1643256753747736\n",
      "Spearman's ρ for path = /simlex-999.txt, context_vocab = /vocab-5k.txt, w = 6, method = tf-idf: 0.1106186938441448\n"
     ]
    }
   ],
   "source": [
    "for path in word_pairs_paths:\n",
    "    word_pairs, word_pair_scores = make_word_pairs(path)\n",
    "\n",
    "    for context_vocab in context_vocab_paths:\n",
    "        vocab_context = load_file(context_vocab)\n",
    "\n",
    "        for w in context_window_sizes:\n",
    "            word_vectors = defaultdict(dict)\n",
    "            vocab = load_file(\"/vocab-15kws.txt\")\n",
    "            my_dict = word_vector_count(w, vocab, vocab_context, wiki_lines)\n",
    "            total_lines, count_context_word_lines = count_lines(set(vocab_context), wiki_lines)\n",
    "            for (word, context_word), count in my_dict.items():\n",
    "                idf = (total_lines / count_context_word_lines[context_word]) if count_context_word_lines[context_word] > 0 else 0\n",
    "                word_vectors[word][context_word] = count * idf\n",
    "\n",
    "\n",
    "            cosine_similarities = {pair: cosine_similarity(word_vectors.get(pair[0], {}), word_vectors.get(pair[1], {}))\n",
    "                                   for pair in word_pairs}\n",
    "\n",
    "            spearman_rho = spearman_rank_correlation(cosine_similarities, word_pair_scores)\n",
    "            print(f\"Spearman's ρ for path = {path}, context_vocab = {context_vocab}, w = {w}, method = tf-idf: {spearman_rho}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "4.1) For PMI method"
   ],
   "metadata": {
    "id": "ktMML2Wt8Zj2"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q3Mx6ccrBRw8",
    "outputId": "89572b54-228a-4147-ab4d-ad9ec2fd0337"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman's ρ for path = /men.txt, context_vocab = /vocab-15kws.txt, w = 1, method = pmi: 0.4703925852658428\n",
      "Spearman's ρ for path = /men.txt, context_vocab = /vocab-15kws.txt, w = 3, method = pmi: 0.519534365170485\n",
      "Spearman's ρ for path = /men.txt, context_vocab = /vocab-15kws.txt, w = 6, method = pmi: 0.5275549925061103\n",
      "Spearman's ρ for path = /men.txt, context_vocab = /vocab-5k.txt, w = 1, method = pmi: 0.43376961586329066\n",
      "Spearman's ρ for path = /men.txt, context_vocab = /vocab-5k.txt, w = 3, method = pmi: 0.46578947742105303\n",
      "Spearman's ρ for path = /men.txt, context_vocab = /vocab-5k.txt, w = 6, method = pmi: 0.4725634710626079\n",
      "Spearman's ρ for path = /simlex-999.txt, context_vocab = /vocab-15kws.txt, w = 1, method = pmi: 0.26807761167981614\n",
      "Spearman's ρ for path = /simlex-999.txt, context_vocab = /vocab-15kws.txt, w = 3, method = pmi: 0.2123053103203404\n",
      "Spearman's ρ for path = /simlex-999.txt, context_vocab = /vocab-15kws.txt, w = 6, method = pmi: 0.16092585471242782\n",
      "Spearman's ρ for path = /simlex-999.txt, context_vocab = /vocab-5k.txt, w = 1, method = pmi: 0.22751080238555188\n",
      "Spearman's ρ for path = /simlex-999.txt, context_vocab = /vocab-5k.txt, w = 3, method = pmi: 0.18644573531447284\n",
      "Spearman's ρ for path = /simlex-999.txt, context_vocab = /vocab-5k.txt, w = 6, method = pmi: 0.1503457936894811\n"
     ]
    }
   ],
   "source": [
    "for path in word_pairs_paths:\n",
    "    word_pairs, word_pair_scores = make_word_pairs(path)\n",
    "\n",
    "    for context_vocab in context_vocab_paths:\n",
    "        vocab_context = load_file(context_vocab)\n",
    "\n",
    "        for w in context_window_sizes:\n",
    "            word_vectors = defaultdict(dict)\n",
    "            vocab = load_file(\"/vocab-15kws.txt\")\n",
    "            my_dict = word_vector_count(w, vocab, vocab_context, wiki_lines)\n",
    "            total_count_N = sum(my_dict.values())\n",
    "            joint_probabilities = {pair: count / total_count_N for pair, count in my_dict.items()}\n",
    "\n",
    "            partial_prob_x = {word: sum(joint_probabilities.get((word, cw), 0) for cw in set(vocab_context)) for word in set(vocab)}\n",
    "            partial_prob_y = {cw: sum(joint_probabilities.get((word, cw), 0) for word in set(vocab)) for cw in set(vocab_context)}\n",
    "\n",
    "            for (word, context_word), joint_prob in joint_probabilities.items():\n",
    "              pmi = math.log2(joint_prob / (partial_prob_x[word] * partial_prob_y[context_word])) if joint_prob > 0 else 0\n",
    "              word_vectors[word][context_word] = pmi\n",
    "\n",
    "\n",
    "            cosine_similarities = {pair: cosine_similarity(word_vectors.get(pair[0], {}), word_vectors.get(pair[1], {}))\n",
    "                                   for pair in word_pairs}\n",
    "\n",
    "            spearman_rho = spearman_rank_correlation(cosine_similarities, word_pair_scores)\n",
    "            print(f\"Spearman's ρ for path = {path}, context_vocab = {context_vocab}, w = {w}, method = pmi: {spearman_rho}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
